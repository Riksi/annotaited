---
layout: post
title:  "IODINE"
date:   2021-11-04 10:41:00 +0100
categories: jekyll update
---

```python 
class IODINE(snt.AbstractModule):
```


We represent each scene with $K$ latent object representations $\mathbf{z}_k \in \mathbb{R}^M$ that collaborate to generate the input image $\mathbf{x} \in \mathbb{R}^D$.

```
num_components (int): Number of available object slots (K).
```

```python
def __init__(...)
...
self.num_components = num_components
...
with tf.name_scope("prior"):
        self.prior = self.latent_dist.get_default_prior((self.num_components,))
```

The image $\mathbf{x}$ is modeled with a spatial Gaussian mixture
model where each mixing component (slot) corresponds
to a single object. That means each object vector $\mathbf{z}_k$ is
decoded into a pixel-wise mean $\mu_{ik}$ (the appearance of the
object) and a pixel-wise assignment $m_{ik} = p(C = k|\mathbf{z}_k)$

## Generative model

> The image $\mathbf{x}$ is modeled with a spatial Gaussian mixture model where each mixing component (slot) corresponds to a single object.

> That means each object vector $\mathbf{z}_k$ is
decoded into a pixel-wise mean $\mu_{ik}$ (the appearance of the
object) and a pixel-wise assignment $m_{ik} = p(C = k|\mathbf{z}_k)$(the segmentation mask...).

- TODO: maybe put mixture here

## Decoder network

```class ComponentDecoder(snt.AbstractModule):```

> Our decoder network structure directly reflects the structure of the generative model. See
Figure 2d for an illustration. 



> Each object latent $\mathbf{z}_k$ is decoded separately into pixel-wise means $\boldsymbol{mu}_k$ and mask-logits
$\mathbf{\hat{m}}_k$, 

```python
pixel_params = self._pixel_decoder(z_flat).params
...
mask_params = pixel_params[..., 0:1]
pixel_params = pixel_params[..., 1:]
```

which we then normalize using a softmax operation applied across slots such that the masks $\mathbf{m}_k$ for each pixel sum
to 1. Together, $\boldsymbol{mu}$ and $\mathbf{m}$ parameterize the spatial mixture distribution as defined in Equation (1).

```
[Config]
"output_dist": {
          "constructor": "iodine.modules.distributions.MaskedMixture"
...}
```

```python
out_dist = self.output_dist(*params)
```

- [ ] What is going on within this distribution?

As for `_pixel_decoder`

> For the network architecture
we use a broadcast decoder ...
which spatially replicates the latent vector zk, appends two
coordinate channels (ranging from -1 to 1 horizontally and
vertically), and applies a series of size-preserving convolutional
layers. This structure encourages disentangling
the position across the image from other features such as
color or texture, and generally supports disentangling.

```
[Config]
"pixel_decoder"{
    "constructor": "iodine.modules.networks.BroadcastConv"
}
```

```
class BroadcastConv(snt.AbstractModule):
  """MLP followed by a broadcast convolution.
  """
```

> `This decoder takes a latent vector z,`

```python
flat_z, unflatten = flatten_all_but_last(z)
```

> `(optionally) applies an MLP to it,`

```python
if self._mlp_opt is None:
      mlp = tf.identity
    else:
      mlp = snt.nets.MLP(activate_final=True, **self._mlp_opt)
    mlp_output = sg.guard(mlp(flat_z), "B, hidden")
```

> `then tiles the resulting vector across space to have dimension [B, H, W, C] i.e. tiles across H and W.`

```python
# tile MLP output spatially and append coordinate channels
    broadcast_mlp_output = tf.tile(
        mlp_output[:, tf.newaxis, tf.newaxis],
        multiples=tf.constant(sg["1, H, W, 1"]),
    )  # B, H, W, Z
```

> `Then coordinate channels are appended`

```python
dec_cnn_inputs = self.append_coordinate_channels(broadcast_mlp_output)
```

within the `append_coordinate_channels` function itself

```python
w_coords = tf.linspace(-1.0, 1.0, sg.W)[None, None, :, None]
      h_coords = tf.linspace(-1.0, 1.0, sg.H)[None, :, None, None]
      w_coords = tf.tile(w_coords, sg["B, H, 1, 1"])
      h_coords = tf.tile(h_coords, sg["B, 1, W, 1"])
      return tf.concat([output, h_coords, w_coords], axis=-1)
```

(the paper only mentions ordinary coordinates but the function also has the option of using cosine positional embeddings) 


> `and a convolutional layer is applied.`

```
cnn = snt.nets.ConvNet2D(
        paddings=("SAME",), normalize_final=False, **self._cnn_opt)
    cnn_outputs = cnn(dec_cnn_inputs)
```

## Inference 

> The basic idea of iterative inference is to start with an arbitrary guess for the posterior parameters, and then iteratively refine them using the input and samples from the current posterior estimate. We build on the framework of iterative amortized inference ... which uses a trained refinement network $f_\phi$.

> We update
the posterior of the K slots independently and in parallel ... as follows

$$\mathbf{z}_k^{(t)} \overset{\sim}{k} q_\lambda(\mathbf{z}_k^{(t)\vert\mathbf{x})\\\\\\
\boldsymbol{\lambda}\_k^{(t+1)} \overset{\leftarrow}{k} \boldsymbol{\lambda}\_k^{(t)} + f_\phi(\mathbf{z}_k, \mathbf{x}, \mathbf{a}_k)$$

> Instead of amortizing the posterior directly (as in a regular
VAE encoder), the refinement network can be thought of
as amortizing the gradient of the posterior. 


For the first step

```zp, z_dist, z = self._get_initial_z()
```
After this the iterative process updates parameters and then samples subsequent latents

```python
for t in range(self.num_iters):
      img = sg.guard(self._get_image_for_iter(images, t), "B, 1, H, W, C")
      x_params, x_dist = self.decode(z) 
      ... 
      zp, state = self.refinement_core(inputs, state)
      sg.guard(zp, "B, K, Zp")

      z_dist = sg.guard(self.latent_dist(zp), "B, K, Z")
      z = z_dist.sample()
```

```
[config]
  "latent_dist": {
          "constructor": "iodine.modules.distributions.LocScaleDistribution",
          "dist": "normal",
          "scale_act": "softplus",
          "scale": "var",
          "name": "latent_dist",
      }
```

- TODO: which steps correspond to equations (2), (3)?

> As refinement network $f_\phi$ we use a convolutional network followed by an LSTM

```
"refinement_core": {
          "constructor": "iodine.modules.refinement.RefinementCore",
          "encoder_net": {
              "constructor": "iodine.modules.networks.CNN",
              ...
          },
          "recurrent_net": {
              "constructor": "iodine.modules.networks.LSTM",
              ...
          },
          "refinement_head": {
              "constructor": "iodine.modules.refinement.ResHead"
          },
      }
```

- [ ] TODO: Add table here

```
[Config]
"cnn_opt": {
                  "output_channels": [64, 64, 64, 64],
                  "strides": [2],
                  "kernel_shapes": [3],
                  "activation": "elu",
              }
```

### CNN

A straightforward conv net

```python
class CNN(snt.AbstractModule):
  """ConvNet2D followed by an MLP.
```

What it does

```python
flat_image, unflatten = flatten_all_but_last(image, n_dims=3)
sg.guard(flat_image, "B, H, W, C")

cnn = snt.nets.ConvNet2D(
    activate_final=True,
    paddings=("SAME",),
    normalize_final=False,
    **self._cnn_opt)
mlp = snt.nets.MLP(**self._mlp_opt)

# run CNN
net = cnn(flat_image)

if self._mode == "flatten":
    # flatten
    net_shape = net.get_shape().as_list()
    flat_shape = net_shape[:-3] + [np.prod(net_shape[-3:])]
    net = tf.reshape(net, flat_shape)
elif self._mode == "avg_pool":
    net = tf.reduce_mean(net, axis=[1, 2])
else:
    raise KeyError('Unknown mode "{}"'.format(self._mode))
# run MLP
output = sg.guard(mlp(net), "B, Y")
```

The function `flatten_all_but_last` flattens all but the last 3 dims so `(B, X, Y, H, W, C)` with `ndims=3` would become `(B*X*Y, H, W, C)`

```python
flat_tensor = tf.reshape(tensor, [np.prod(batch_dims)] + shape[-n_dims:])
```

It also returns an `unflatten` function that undoes this transform.

### LSTM

A straightforward stacked LSTM with hidden size `h`:

```class LSTM(snt.RNNCore):
  """Wrapper around snt.LSTM that supports multi-layers and runs K components in
  parallel.
  Expects input data of shape (B, K, H) and outputs data of shape (B, K, Y)
  """
```

This is what it does

```python
data = sg.reshape(data, "B*K, H")

out = data
new_states = []
for lstm, pstate in zip(self._lstm_layers, prev_states):
    out, nstate = lstm(out, pstate)
    new_states.append(nstate)

sg.guard(out, "B*K, Y")
out = sg.reshape(out, "B, K, Y")
```

## Inputs to $f_\phi$

